# agentiso configuration
#
# Copy this file to config.toml and uncomment/modify values as needed.
# All values shown are the compiled-in defaults.
#
# Usage:
#   agentiso serve --config config.toml
#   agentiso check --config config.toml
#   agentiso status --config config.toml
#   agentiso logs <workspace-id> --config config.toml

# ---------------------------------------------------------------------------
# Server / daemon settings
# ---------------------------------------------------------------------------
[server]

# Path to persist workspace state as JSON. The daemon writes this file
# periodically and on shutdown; CLI commands (status, logs) read it.
# state_file = "/var/lib/agentiso/state.json"

# How often (in seconds) to persist in-memory state to the state file.
# state_persist_interval_secs = 30

# Allowed directory for host-side file transfers (upload/download).
# All host_path values in file_transfer MCP tool must resolve within
# this directory. Prevents path-traversal attacks.
# transfer_dir = "/var/lib/agentiso/transfers"

# ---------------------------------------------------------------------------
# VM / QEMU settings
# ---------------------------------------------------------------------------
[vm]

# Path to the kernel image (bzImage or vmlinux).
# kernel_path = "/var/lib/agentiso/vmlinuz"

# Optional path to the initramfs image. Required for distro kernels that
# have virtio drivers compiled as modules rather than built-in.
# initrd_path = "/var/lib/agentiso/initrd.img"

# Path to the qemu-system-x86_64 binary.
# qemu_binary = "qemu-system-x86_64"

# Runtime directory for per-workspace QMP sockets, PID files, and logs.
# run_dir = "/run/agentiso"

# Kernel boot arguments passed to QEMU via -append.
# kernel_append = "console=ttyS0 root=/dev/vda rw quiet"

# Starting vsock CID. Each new workspace gets the next available CID.
# Must be >= 3 (0-2 are reserved by the vsock protocol).
# vsock_cid_start = 100

# Vsock port the guest agent listens on inside each VM.
# guest_agent_port = 5000

# Timeout in seconds for the guest agent readiness handshake after VM boot.
# If the guest agent does not respond within this time, workspace creation fails.
# boot_timeout_secs = 30

# Init mode inside the guest VM.
#   "openrc" — standard Alpine OpenRC init (slower, full service manager)
#   "fast"   — minimal init shim that launches the guest agent directly (faster boot)
# init_mode = "openrc"

# Optional path to a fast initrd image, used when init_mode = "fast".
# This should be a minimal initramfs that boots directly into the guest agent.
# initrd_fast_path = ""

# ---------------------------------------------------------------------------
# ZFS storage settings
# ---------------------------------------------------------------------------
[storage]

# ZFS pool name.
# zfs_pool = "agentiso"

# Dataset prefix under the pool. The full layout is:
#   {zfs_pool}/{dataset_prefix}/base/{base_image}       — base images
#   {zfs_pool}/{dataset_prefix}/workspaces/ws-{id}      — workspace zvols
#   {zfs_pool}/{dataset_prefix}/forks/...               — forked workspaces
# dataset_prefix = "agentiso"

# Name of the base image dataset (under base/).
# base_image = "alpine-dev"

# Snapshot name on the base image that new workspaces are cloned from.
# base_snapshot = "latest"

# ---------------------------------------------------------------------------
# Network settings
# ---------------------------------------------------------------------------
[network]

# Bridge device name. Created by the setup script; must exist before serving.
# bridge_name = "br-agentiso"

# Gateway IP on the bridge (host side). This is the default gateway inside VMs.
# gateway_ip = "10.99.0.1"

# Subnet prefix length (e.g. 16 for a /16 network = 65534 addresses).
# Must be between 8 and 30.
# subnet_prefix = 16

# Whether new workspaces can reach the internet by default.
# Can be overridden per-workspace via MCP tools.
# default_allow_internet = true

# Whether new workspaces can communicate with other VMs by default.
# Can be overridden per-workspace via MCP tools.
# default_allow_inter_vm = false

# DNS servers configured inside guest VMs (written to /etc/resolv.conf).
# dns_servers = ["1.1.1.1", "8.8.8.8"]

# ---------------------------------------------------------------------------
# Default resource limits for workspaces
# ---------------------------------------------------------------------------
[resources]

# Default vCPUs allocated to each new workspace.
# default_vcpus = 2

# Default memory in MB allocated to each new workspace.
# default_memory_mb = 512

# Default disk size in GB for each new workspace zvol.
# default_disk_gb = 10

# Maximum vCPUs a single workspace can request.
# max_vcpus = 8

# Maximum memory in MB a single workspace can request.
# max_memory_mb = 8192

# Maximum disk in GB a single workspace can request.
# max_disk_gb = 100

# Maximum number of concurrent workspaces the daemon will manage.
# max_workspaces = 20

# When true, workspace creation fails if cgroup v2 setup is unavailable.
# When false (default), cgroup limits are best-effort — workspaces continue
# without resource isolation if cgroups cannot be configured.
# cgroup_required = false

# Maximum number of snapshots per workspace. Prevents unbounded disk growth
# from snapshot accumulation. Delete old snapshots to free up space.
# max_snapshots_per_workspace = 20

# ---------------------------------------------------------------------------
# Warm VM pool (pre-boot VMs for faster workspace creation)
# ---------------------------------------------------------------------------
[pool]

# Enable the warm VM pool. When enabled, the daemon pre-boots VMs in the
# background so that workspace creation can skip the boot wait.
# enabled = true

# Minimum number of VMs to keep in the pool at all times.
# min_size = 2

# Maximum number of VMs allowed in the pool.
# max_size = 10

# Target number of free (ready) VMs the pool tries to maintain.
# The daemon will boot new VMs when free count drops below this.
# target_free = 3

# Total memory budget in MB for all pool VMs combined.
# Prevents the pool from consuming too much host RAM.
# max_memory_mb = 8192

# ---------------------------------------------------------------------------
# Vault — Obsidian-style markdown knowledge base
# ---------------------------------------------------------------------------
[vault]

# Enable the bundled vault tool (read, search, list, write, delete, etc.).
# When disabled, the vault tool is not registered with the MCP server.
# enabled = false

# Path to the vault root directory on the host filesystem.
# All vault operations are confined to this directory (path traversal prevented).
# path = "/mnt/vault"

# File extensions to include in search and list operations.
# extensions = ["md"]

# Directories to exclude from search and list operations.
# These are matched by name at any depth in the vault tree.
# exclude_dirs = [".obsidian", ".trash", ".git"]

# ---------------------------------------------------------------------------
# Rate limiting — token-bucket rate limits for MCP tool calls
# ---------------------------------------------------------------------------
[rate_limit]

# Enable rate limiting. Set to false to disable all rate limits.
# enabled = true

# Maximum workspace-creation calls per minute (workspace_create,
# workspace_fork). These are the most expensive operations since they
# boot VMs and clone ZFS datasets.
# create_per_minute = 5

# Maximum exec-category calls per minute (exec, exec_background).
# Increase this for build/test-heavy workflows.
# exec_per_minute = 60

# Maximum calls per minute for all other tools (file read/write/list,
# snapshot, networking, git, vault, etc.).
# default_per_minute = 120

# ---------------------------------------------------------------------------
# MCP Bridge — HTTP transport for VM-based OpenCode clients
# ---------------------------------------------------------------------------
[mcp_bridge]

# Enable the HTTP MCP bridge. When enabled, the server listens on the bridge
# interface for HTTP-based MCP connections from OpenCode instances inside VMs.
# Each connection is authenticated via a workspace-scoped bearer token.
# enabled = false

# IP address to bind to (should be the bridge gateway IP so VMs can reach it).
# bind_addr = "10.99.0.1"

# TCP port to listen on.
# port = 3100

# Optional ollama port to allow VMs to access local models on the host.
# When set, nftables rules allow VMs to connect to this port on the bridge IP.
# ollama_port = 11434

# ---------------------------------------------------------------------------
# Web Dashboard — React-based management UI
# ---------------------------------------------------------------------------
[dashboard]

# Enable the web dashboard HTTP server. When enabled, serves a React UI
# with REST API, WebSocket, and static file serving.
# enabled = false

# IP address to bind to. Use "127.0.0.1" for localhost-only access,
# or "0.0.0.0" to listen on all interfaces (set admin_token if exposed).
# bind_addr = "127.0.0.1"

# TCP port to listen on.
# port = 8080

# Admin token for API authentication. When empty (default), no auth is
# required — safe when binding to localhost only.
# Set a strong token when binding to non-localhost addresses.
# admin_token = ""

# Path to React build directory for static file serving.
# When empty (default), serves the embedded dashboard build from the binary.
# Set this to override with a local directory (e.g., for development).
# static_dir = ""
